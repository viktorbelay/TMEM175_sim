Sender: LSF System <lsfadmin@lt12>
Subject: Job 2609817: <26 0> in cluster <lila> Exited

Job <26 0> was submitted from host <lilac-ln02> by user <belayv> in cluster <lila> at Tue Aug 24 17:53:47 2021
Job was executed on host(s) <lt12>, in queue <gpuqueue>, as user <belayv> in cluster <lila> at Tue Aug 24 17:53:49 2021
</home/belayv> was used as the home directory.
</data/chodera/viktor/tmem175> was used as the working directory.
Started at Tue Aug 24 17:53:49 2021
Terminated at Tue Aug 24 17:53:51 2021
Results reported at Tue Aug 24 17:53:51 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash
# Set walltime limit
#BSUB -W 24:00
#
# Set output file
#BSUB -o  md.26.0.out
#
# Set error file
#BSUB -eo md.26.0.stderr
#
# Specify node group
#BSUB -m "lu-gpu lv-gpu ld-gpu lt-gpu lp-gpu lg-gpu boson"
#BSUB -q gpuqueue
#
# nodes: number of nodes and GPU request
#BSUB -n 1 -R "rusage[mem=12]"
#BSUB -gpu "num=1:j_exclusive=yes:mode=shared"
#
# job name (default = name of script file)
#BSUB -J "26 0"
source ~/.bashrc
conda activate simenv2
module load cuda/10.1
python testpy.py "/data/chodera/viktor/tmem175/"

------------------------------------------------------------

Exited with exit code 2.

Resource usage summary:

    CPU time :                                   0.52 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     12.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   3 sec.
    Turnaround time :                            4 sec.

The output (if any) follows:



PS:

Read file <md.26.0.stderr> for stderr output of this job.

Sender: LSF System <lsfadmin@lt12>
Subject: Job 2609818: <26 0> in cluster <lila> Exited

Job <26 0> was submitted from host <lilac-ln02> by user <belayv> in cluster <lila> at Tue Aug 24 17:54:53 2021
Job was executed on host(s) <lt12>, in queue <gpuqueue>, as user <belayv> in cluster <lila> at Tue Aug 24 17:54:55 2021
</home/belayv> was used as the home directory.
</data/chodera/viktor/tmem175> was used as the working directory.
Started at Tue Aug 24 17:54:55 2021
Terminated at Tue Aug 24 17:55:03 2021
Results reported at Tue Aug 24 17:55:03 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash
# Set walltime limit
#BSUB -W 24:00
#
# Set output file
#BSUB -o  md.26.0.out
#
# Set error file
#BSUB -eo md.26.0.stderr
#
# Specify node group
#BSUB -m "lu-gpu lv-gpu ld-gpu lt-gpu lp-gpu lg-gpu boson"
#BSUB -q gpuqueue
#
# nodes: number of nodes and GPU request
#BSUB -n 1 -R "rusage[mem=12]"
#BSUB -gpu "num=1:j_exclusive=yes:mode=shared"
#
# job name (default = name of script file)
#BSUB -J "26 0"
source ~/.bashrc
conda activate simenv2
module load cuda/10.1
python sim.py "/data/chodera/viktor/tmem175/"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1.41 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     12.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   9 sec.
    Turnaround time :                            10 sec.

The output (if any) follows:



PS:

Read file <md.26.0.stderr> for stderr output of this job.

Sender: LSF System <lsfadmin@lt08>
Subject: Job 2609819: <26 0> in cluster <lila> Exited

Job <26 0> was submitted from host <lilac-ln02> by user <belayv> in cluster <lila> at Tue Aug 24 18:01:30 2021
Job was executed on host(s) <lt08>, in queue <gpuqueue>, as user <belayv> in cluster <lila> at Tue Aug 24 18:01:32 2021
</home/belayv> was used as the home directory.
</data/chodera/viktor/tmem175> was used as the working directory.
Started at Tue Aug 24 18:01:32 2021
Terminated at Tue Aug 24 18:01:38 2021
Results reported at Tue Aug 24 18:01:38 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash
# Set walltime limit
#BSUB -W 24:00
#
# Set output file
#BSUB -o  md.26.0.out
#
# Set error file
#BSUB -eo md.26.0.stderr
#
# Specify node group
#BSUB -m "lu-gpu lv-gpu ld-gpu lt-gpu lp-gpu lg-gpu boson"
#BSUB -q gpuqueue
#
# nodes: number of nodes and GPU request
#BSUB -n 1 -R "rusage[mem=12]"
#BSUB -gpu "num=1:j_exclusive=yes:mode=shared"
#
# job name (default = name of script file)
#BSUB -J "26 0"
source ~/.bashrc
conda activate simenv2
module load cuda/10.1
python sim.py "/data/chodera/viktor/tmem175/"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1.27 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     12.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   7 sec.
    Turnaround time :                            8 sec.

The output (if any) follows:



PS:

Read file <md.26.0.stderr> for stderr output of this job.

Sender: LSF System <lsfadmin@lt12>
Subject: Job 2609824: <26 0> in cluster <lila> Exited

Job <26 0> was submitted from host <lilac-ln02> by user <belayv> in cluster <lila> at Tue Aug 24 18:07:28 2021
Job was executed on host(s) <lt12>, in queue <gpuqueue>, as user <belayv> in cluster <lila> at Tue Aug 24 18:07:31 2021
</home/belayv> was used as the home directory.
</data/chodera/viktor/tmem175> was used as the working directory.
Started at Tue Aug 24 18:07:31 2021
Terminated at Tue Aug 24 18:07:36 2021
Results reported at Tue Aug 24 18:07:36 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash
# Set walltime limit
#BSUB -W 24:00
#
# Set output file
#BSUB -o  md.26.0.out
#
# Set error file
#BSUB -eo md.26.0.stderr
#
# Specify node group
#BSUB -m "lu-gpu lv-gpu ld-gpu lt-gpu lp-gpu lg-gpu boson"
#BSUB -q gpuqueue
#
# nodes: number of nodes and GPU request
#BSUB -n 1 -R "rusage[mem=12]"
#BSUB -gpu "num=1:j_exclusive=yes:mode=shared"
#
# job name (default = name of script file)
#BSUB -J "26 0"
source ~/.bashrc
conda activate simenv2
module load cuda/10.1
python sim.py "/data/chodera/viktor/tmem175/"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1.36 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     12.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   6 sec.
    Turnaround time :                            8 sec.

The output (if any) follows:



PS:

Read file <md.26.0.stderr> for stderr output of this job.

Sender: LSF System <lsfadmin@lt12>
Subject: Job 2609827: <26 0> in cluster <lila> Exited

Job <26 0> was submitted from host <lilac-ln02> by user <belayv> in cluster <lila> at Tue Aug 24 18:10:22 2021
Job was executed on host(s) <lt12>, in queue <gpuqueue>, as user <belayv> in cluster <lila> at Tue Aug 24 18:10:25 2021
</home/belayv> was used as the home directory.
</data/chodera/viktor/tmem175> was used as the working directory.
Started at Tue Aug 24 18:10:25 2021
Terminated at Tue Aug 24 18:10:32 2021
Results reported at Tue Aug 24 18:10:32 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash
# Set walltime limit
#BSUB -W 24:00
#
# Set output file
#BSUB -o  md.26.0.out
#
# Set error file
#BSUB -eo md.26.0.stderr
#
# Specify node group
#BSUB -m "lu-gpu lv-gpu ld-gpu lt-gpu lp-gpu lg-gpu boson"
#BSUB -q gpuqueue
#
# nodes: number of nodes and GPU request
#BSUB -n 1 -R "rusage[mem=12]"
#BSUB -gpu "num=1:j_exclusive=yes:mode=shared"
#
# job name (default = name of script file)
#BSUB -J "26 0"
source ~/.bashrc
conda activate simenv2
module load cuda/10.1
python sim.py "/data/chodera/viktor/tmem175/"

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   2.05 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     12.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                13
    Run time :                                   9 sec.
    Turnaround time :                            10 sec.

The output (if any) follows:



PS:

Read file <md.26.0.stderr> for stderr output of this job.

Sender: LSF System <lsfadmin@lt01>
Subject: Job 2609832: <26 0> in cluster <lila> Done

Job <26 0> was submitted from host <lilac-ln02> by user <belayv> in cluster <lila> at Tue Aug 24 18:12:09 2021
Job was executed on host(s) <lt01>, in queue <gpuqueue>, as user <belayv> in cluster <lila> at Tue Aug 24 18:12:13 2021
</home/belayv> was used as the home directory.
</data/chodera/viktor/tmem175> was used as the working directory.
Started at Tue Aug 24 18:12:13 2021
Terminated at Tue Aug 24 18:34:41 2021
Results reported at Tue Aug 24 18:34:41 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash
# Set walltime limit
#BSUB -W 24:00
#
# Set output file
#BSUB -o  md.26.0.out
#
# Set error file
#BSUB -eo md.26.0.stderr
#
# Specify node group
#BSUB -m "lu-gpu lv-gpu ld-gpu lt-gpu lp-gpu lg-gpu boson"
#BSUB -q gpuqueue
#
# nodes: number of nodes and GPU request
#BSUB -n 1 -R "rusage[mem=12]"
#BSUB -gpu "num=1:j_exclusive=yes:mode=shared"
#
# job name (default = name of script file)
#BSUB -J "26 0"
source ~/.bashrc
conda activate simenv2
module load cuda/10.1
python sim.py "/data/chodera/viktor/tmem175/"

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1336.91 sec.
    Max Memory :                                 3 GB
    Average Memory :                             1.95 GB
    Total Requested Memory :                     12.00 GB
    Delta Memory :                               9.00 GB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                234
    Run time :                                   1348 sec.
    Turnaround time :                            1352 sec.

The output (if any) follows:

  initial : 289249.067 kcal/mol
  final : -271308.234 kcal/mol
Serializing state to minimized-state.xml
Serializing system to minimized-system.xml
Serializing integrator to minimized-integrator.xml
Writing system to minimized.pdb
Generating random starting velocities
Running dynamics in the NPT ensemble...
#"Progress (%)"	"Step"	"Time (ps)"	"Potential Energy (kJ/mole)"	"Kinetic Energy (kJ/mole)"	"Temperature (K)"	"Speed (ns/day)"	"Time Remaining"
20.0%	50000	199.9999999998967	-836322.4018459925	328356.91130854294	304.5444549909505	0	--
40.0%	100000	400.0000000004537	-837322.4018459925	328289.4855428731	304.48191894449894	89.5	9:39
60.0%	150000	600.0000000014087	-839628.4018459925	328044.898048275	304.25506894477417	89.2	6:27
80.0%	200000	800.0000000023637	-838790.4018459925	326765.4821272376	303.0684363174105	89.1	3:14
100.0%	250000	1000.0000000033186	-842047.4018459925	328713.25468911923	304.8749563353406	88.9	0:00
    Equilibration took 969.146 s for 1.000 ns (  89.151 ns/day)
Serializing state to state.xml
Saving final state as equilibrated.pdb
Serializing system to system.xml
Serializing integrator to integrator.xml


PS:

Read file <md.26.0.stderr> for stderr output of this job.

